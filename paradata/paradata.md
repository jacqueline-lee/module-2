For Module 2, we were required to create a new digital object, remixing our own models or a classmate’s. I chose to remix my own 3D model of the Mayfair Theatre by sonifying parts of the numerical data from the .obj file.  

After Module 1, I realized that while it was really convenient to use my friend Martazia’s equipment and software to create the scans for the 3D model, it left me with a much larger model and probably many, many more data points in my one object than most of my other classmates. I figured it would probably be a better choice to scale down my ambitions for the rest of the projects so the amount of data I did have to work with subsequently would be of a more manageable size.  

I followed the instructions on the FAQ page for our class on how to sonify data. I downloaded Sublime Text as suggested, and loaded the Mayfair Theatre model .obj file into it. This was an interesting experience because it is such a massive file that my computer was not really able to handle the amount of data being processed along with everything else I had open. I am one of those people who always has at least a dozen or two tabs open in my browser at any given time, plus excel files and word documents and outlook and all sorts of programs running at the same time. Eventually it loaded, but it was very difficult to navigate as it had to continually reload more data, and I decided to leave it and come back to it another day. However, before leaving it for the time being, I decided to go and download the model being used in the example, a chess Queen from the British Museum, and open it as a text file just to compare how much more material I had to work with. The model of the queen had over 18 million lines of data, while the Mayfair Theatre had nearly 65 million. This definitely made me realize just how much more data I had compiled in Module 1 due to the size and scale of my model.  

When I did eventually come back to the data, I closed almost everything else that I was running at the same time. I closed most of my browser tabs, leaving open the instructions for sonifying data, among a few others, but almost every other program was shut down before I tried to open the text version of the .obj file. Even then, Sublime Text froze a few times before it finally loaded everything. I was unsure what they meant but there seemed to be points in the data where the numerical values would shift dramatically. I knew I would not be able to sonify all 65 million lines of data so I decided to just copy segments from the top. These data shift points seemed to be as good as any other for dividing up the data that I was moving into an Excel spreadsheet for easier access and data manipulation. I collected 26 of these segments, a total of 1694 lines of data. This was an arbitrary amount that I picked, and each segment had varying amounts of data within them. I did not expect to be able to sonify all of the data I had extracted, nor did I have an idea of where in the model these data points correlated to.   

I did not feel like it was within the scope of this project and course for me to take the time to figure out exactly where these points belonged, but it did feel a little bit like the way museums in the 19th century make kind of a random grab for antiquities without documenting the circumstances under which they were found. I knew generally that they were a part of the model but I had no idea which of the twelve original scans they were, or if it was a point that was overlapped by several scans or one of the far corners of the theatre only picked up by one or two scans.   

I also only had a loose understanding of how Music Algorithms worked. I knew that if I put the numerical data in, sounds would come out, but I didn’t understand how exactly the numbers were shifting as I went through the different steps of choices.   

I also realized that as I started manipulating the sounds I received at the output end of the Music Algorithms process (I am unwilling to call it “music”) that perhaps someone else would interpret and alter this barrage of noise to be more pleasing to listen to differently, and have subsequently uploaded the original raw noise that came out of Music Algorithms. I stand by the alterations I made because as much as we would like to think the artifacts in museums to be unchanged by the curation process, I think the way they are presented and what other artifacts they are juxtaposed with can change the interpretation the viewer takes away, and is an expression of the curator’s perspective on that particular artifact. While my manipulation is not an objective improvement, I think the idea that exhibits and artifacts we create can be objective is not possible and we need to address that. I manipulated the music in the way I did because I learned to read sheet music and I based my manipulations on the sheet music produced by Score Cloud, because I thought using sheet music would be easier than trying to manipulate sound waves in Audacity or a similar program. I would also like to think the sounds I thought would be more pleasing to hear are closer to classical music because of the influence classical music has had on my life, both growing up and in my volunteer work over the past few years with Chamberfest.   

Ultimately, I think the artifact I created for Module 2 is much more subjective than the 3D model I created for Module 1, but I do not think that makes it a worse result. I think everything we experience is subjective and if we can keep that in mind and keep our biases and the biases of the people curating what we see and experience, especially in museums or other venues that purport to be “objective” in presenting facts and the truth, we can learn much more and deeper than a surface level absorption of facts.

